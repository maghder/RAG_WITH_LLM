# ============================================================================
# Application de Questions-R√©ponses pour manuels de proc√©dures
# ============================================================================
# Utilise Gradio pour l'interface utilisateur
# Int√®gre Ollama pour les embeddings et le mod√®le LLM
# Utilise Chroma comme base de donn√©es vectorielle
# ============================================================================

import gradio as gr
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from datetime import datetime
from pathlib import Path
import json
try:
    from reportlab.lib.pagesizes import letter
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
    from reportlab.lib.units import inch
    from reportlab.lib import colors
    REPORTLAB_AVAILABLE = True
except ImportError:
    REPORTLAB_AVAILABLE = False

# Historique de la conversation pour la m√©moire contextuelle
conversation_history = []

# Dossier pour les exports
export_folder = Path("exports")
export_folder.mkdir(exist_ok=True)

# ============================================================================
# INITIALISATION DES COMPOSANTS
# ============================================================================

# Initialiser les embeddings avec le mod√®le mxbai-embed-large
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

# Charger la base de donn√©es vectorielle Chroma existante
vector_db = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings,
    collection_name="documents"
)

# Initialiser le mod√®le LLM Mistral
llm = OllamaLLM(model="mistral")

# ============================================================================
# CONFIGURATION DU PROMPT ET DE LA CHA√éNE RAG
# ============================================================================

# Cr√©er le mod√®le de prompt avec historique de conversation
prompt = ChatPromptTemplate.from_template(
    """Tu es un assistant expert charg√© d'interroger un manuel de proc√©dures.
Utilise uniquement le CONTEXTE fourni pour r√©pondre (extraits, paragraphes, sections).
Ne devine pas en dehors du contexte ; si l'information n'appara√Æt pas dans le contexte, indique clairement
que l'information n'a pas √©t√© trouv√©e et propose des termes ou sections √† rechercher.

Consignes de r√©ponse :
- R√©sum√© concis (4-6 phrases) de la r√©ponse.
- Si la question demande une proc√©dure, fournis une liste NUM√âROT√âE des √©tapes √† suivre.
- Apr√®s la r√©ponse, ajoute une section "Sources" listant les fichiers/sections utilis√©s (format : `NomFichier: [page/section]` si disponible).
- Indique un court niveau de confiance (√âlev√© / Moyen / Faible) et la raison.
- Si la question est ambigu√´ ou il manque des pr√©cisions, pose jusqu'√† 2 questions de clarification.
- R√©ponds en fran√ßais.

Contexte: {context}

Historique de conversation:
{history}

Question: {question}

R√©ponse:"""
)

# Initialiser le r√©cup√©rateur avec top-3 r√©sultats similaires
retriever = vector_db.as_retriever(search_kwargs={"k": 5})

# Fonction pour formater les documents r√©cup√©r√©s
def format_docs(docs):
    """Joindre les contenus des documents avec s√©paration"""
    return "\n\n".join(doc.page_content for doc in docs)

# Fonction pour formater l'historique de conversation
def format_history():
    """Convertir l'historique en cha√Æne lisible pour le prompt"""
    if not conversation_history:
        return "Aucune conversation pr√©c√©dente."
    history_text = ""
    # Garder les 4 derniers messages pour le contexte
    for msg in conversation_history[-4:]:
        if isinstance(msg, dict):
            role = msg["role"]
            content = msg["content"]
            history_text += f"{role}: {content}\n"
    return history_text

# Cr√©er la cha√Æne RAG (Retrieval-Augmented Generation)
rag_chain = (
    {
        "context": retriever | format_docs,
        "history": lambda x: format_history(),
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)

# ============================================================================
# FONCTIONS DE TRAITEMENT DES REQU√äTES
# ============================================================================

def query_documents(question):
    """
    Traiter une question et retourner la r√©ponse avec les sources
    
    Args:
        question (str): La question pos√©e par l'utilisateur
        
    Returns:
        str: La r√©ponse format√©e avec les sources utilis√©es
    """
    try:
        # Obtenir la r√©ponse de la cha√Æne RAG
        answer = rag_chain.invoke(question)
        
        # Sauvegarder dans l'historique de conversation
        conversation_history.append({"role": "Utilisateur", "content": question})
        conversation_history.append({"role": "Assistant", "content": answer})
        
        # R√©cup√©rer les documents pertinents (sources)
        docs = retriever.invoke(question)
        sources = list(set([doc.metadata.get("source", "Unknown") for doc in docs]))
        
        # Formater la r√©ponse avec les sources
        response = f"{answer}\n\n---\n\nüìö **Sources utilis√©es:**\n"
        for source in sources:
            response += f"‚Ä¢ {source}\n"
        
        return response
    except Exception as e:
        return f"Erreur: {str(e)}"


def export_to_pdf():
    """
    Exporter la conversation actuelle en PDF
    """
    try:
        if not conversation_history:
            return "Aucune conversation √† exporter."
        
        if not REPORTLAB_AVAILABLE:
            # Utiliser un format texte simple si reportlab n'est pas disponible
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"exports/conversation_{timestamp}.txt"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("EXPORT DE CONVERSATION - MANUEL DE PROC√âDURES\n")
                f.write(f"Date: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
                f.write("=" * 80 + "\n\n")
                
                for msg in conversation_history:
                    role = msg.get("role", "Unknown")
                    content = msg.get("content", "")
                    f.write(f"\n{role}:\n")
                    f.write("-" * 40 + "\n")
                    f.write(content)
                    f.write("\n")
            
            return f"‚úÖ Conversation export√©e en texte: {filename}"
        
        # Exportation PDF avec reportlab
        from reportlab.lib.pagesizes import letter
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
        from reportlab.lib.units import inch
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"exports/conversation_{timestamp}.pdf"
        
        # Cr√©er le document PDF
        doc = SimpleDocTemplate(filename, pagesize=letter)
        styles = getSampleStyleSheet()
        story = []
        
        # En-t√™te
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=16,
            textColor=colors.HexColor('#1f4788'),
            spaceAfter=12,
            alignment=1
        )
        
        story.append(Paragraph("EXPORT DE CONVERSATION", title_style))
        story.append(Paragraph("Syst√®me de Questions-R√©ponses sur les manuels de proc√©dures", styles['Normal']))
        story.append(Paragraph(f"Date: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}", styles['Normal']))
        story.append(Spacer(1, 0.5*inch))
        
        # Contenu de la conversation
        for msg in conversation_history:
            role = msg.get("role", "Unknown")
            content = msg.get("content", "")
            
            # Style bas√© sur le r√¥le
            if role == "Utilisateur":
                msg_style = ParagraphStyle(
                    'UserMessage',
                    parent=styles['Normal'],
                    textColor=colors.HexColor('#0066cc'),
                    fontSize=10,
                    spaceAfter=8,
                    leftIndent=0.2*inch
                )
            else:
                msg_style = ParagraphStyle(
                    'AssistantMessage',
                    parent=styles['Normal'],
                    textColor=colors.HexColor('#006600'),
                    fontSize=10,
                    spaceAfter=8,
                    leftIndent=0.2*inch
                )
            
            story.append(Paragraph(f"<b>{role}:</b>", msg_style))
            # Tronquer les r√©ponses tr√®s longues
            content_short = content[:500] + "..." if len(content) > 500 else content
            story.append(Paragraph(content_short, styles['Normal']))
            story.append(Spacer(1, 0.2*inch))
        
        # G√©n√©rer le PDF
        doc.build(story)
        return f"‚úÖ Conversation export√©e en PDF: {filename}"
    
    except Exception as e:
        return f"‚ùå Erreur lors de l'export: {str(e)}"


def get_document_statistics():
    """
    R√©cup√©rer les statistiques sur les documents dans la base de donn√©es
    """
    try:
        # R√©cup√©rer tous les documents de la collection
        collection = vector_db.get()
        
        if not collection or not collection.get('ids'):
            return "Aucun document dans la base de donn√©es."
        
        # Compter les documents par source
        sources = {}
        metadatas = collection.get('metadatas', [])
        
        for metadata in metadatas:
            source = metadata.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
        
        # Construire le rapport de statistiques
        stats_text = "üìä **STATISTIQUES SUR LES DOCUMENTS**\n\n"
        stats_text += f"Nombre total de chunks: {len(collection.get('ids', []))}\n"
        stats_text += f"Nombre de documents sources: {len(sources)}\n\n"
        stats_text += "**R√©partition par document:**\n"
        
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(collection.get('ids', []))) * 100
            stats_text += f"‚Ä¢ {source}: {count} chunks ({percentage:.1f}%)\n"
        
        return stats_text
    
    except Exception as e:
        return f"Erreur lors de la r√©cup√©ration des statistiques: {str(e)}"

# ============================================================================
# INTERFACE UTILISATEUR GRADIO
# ============================================================================

# Cr√©er l'interface avec Gradio
with gr.Blocks(title="Syst√®me QA - Proc√©dures d'Archivage") as demo:
    gr.Markdown("# üìö Syst√®me de Questions-R√©ponses sur les Proc√©dures d'Archivage")
    gr.Markdown("Posez des questions sur les documents d'archivage. Le syst√®me se souviendra de vos questions pr√©c√©dentes.")
    
    # Section pour la saisie de la question
    with gr.Row():
        question_input = gr.Textbox(
            label="Votre Question",
            placeholder="Tapez votre question ici...",
            lines=3
        )
    
    # Section pour les boutons d'action
    with gr.Row():
        submit_btn = gr.Button("Rechercher", variant="primary")
        clear_btn = gr.Button("Effacer")
        reset_history_btn = gr.Button("R√©initialiser l'historique")
    
    # Section pour l'affichage de la r√©ponse
    with gr.Row():
        answer_output = gr.Textbox(
            label="R√©ponse",
            lines=8,
            interactive=False
        )
    
    # Section pour les fonctionnalit√©s suppl√©mentaires
    with gr.Row():
        export_btn = gr.Button("üì• Exporter en PDF/TXT", variant="secondary")
        stats_btn = gr.Button("üìä Voir les statistiques", variant="secondary")
    
    with gr.Row():
        stats_output = gr.Textbox(
            label="Statistiques / Export",
            lines=6,
            interactive=False
        )
    
    # Fonction pour r√©initialiser l'historique
    def reset_history():
        """Effacer l'historique de conversation"""
        conversation_history.clear()
        return ("", "")
    
    # √âv√©nements des boutons
    submit_btn.click(
        fn=query_documents,
        inputs=question_input,
        outputs=answer_output
    )
    
    clear_btn.click(
        fn=lambda: ("", ""),
        outputs=[question_input, answer_output]
    )
    
    reset_history_btn.click(
        fn=reset_history,
        outputs=[question_input, answer_output]
    )
    
    export_btn.click(
        fn=export_to_pdf,
        outputs=stats_output
    )
    
    stats_btn.click(
        fn=get_document_statistics,
        outputs=stats_output
    )

# ============================================================================
# LANCEMENT DE L'APPLICATION
# ============================================================================

if __name__ == "__main__":
    # Lancer l'application Gradio
    # Accessible √† http://127.0.0.1:7860
    demo.launch(server_name="127.0.0.1", server_port=7860)
