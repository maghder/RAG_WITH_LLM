# ğŸ“š SystÃ¨me de Questions-RÃ©ponses pour manuels de procÃ©dures

Un systÃ¨me intelligent de recherche et de question-rÃ©ponse basÃ© sur l'IA pour interroger des manuels de procÃ©dures gÃ©nÃ©rales. Utilise la gÃ©nÃ©ration augmentÃ©e par retrieval (RAG) avec une mÃ©moire conversationnelle.

## ğŸ¯ FonctionnalitÃ©s

- âœ… **Conversion de documents** - Supporte les fichiers Word (.docx) et PDF (.pdf)
- âœ… **Chunking intelligent** - Utilise le chunking hybride de Docling
- âœ… **Embeddings sÃ©mantiques** - ModÃ¨le mxbai-embed-large d'Ollama
- âœ… **Base de donnÃ©es vectorielle** - Stockage avec Chroma
- âœ… **RÃ©ponses basÃ©es sur le contexte** - ModÃ¨le Mistral d'Ollama
- âœ… **MÃ©moire conversationnelle** - Garde l'historique des questions
- âœ… **Interface utilisateur** - Interface web avec Gradio
- âœ… **Sources citÃ©es** - Affiche les documents utilisÃ©s pour chaque rÃ©ponse
- âœ… **Export en PDF/TXT** - Exporte les conversations
- âœ… **Statistiques** - Affiche les statistiques des documents indexÃ©s

## ğŸ“‹ PrÃ©requis

### Logiciels nÃ©cessaires
- Python 3.9+
- Ollama (avec les modÃ¨les `mxbai-embed-large` et `mistral` installÃ©s)

### Installation des dÃ©pendances

```bash
pip install langchain-ollama langchain-chroma gradio docling python-docx reportlab
```

## ğŸš€ Guide de dÃ©marrage

### Ã‰tape 1: PrÃ©parer les documents

1. Placez vos fichiers Word (.docx) ou PDF (.pdf) dans le dossier `docs-procedure/`
2. Exemple de fichiers supportÃ©s:
   - `procedure_generale.docx`
   - `securite_operationnelle.docx`
   - `maintenance_etalonage.pdf`
   - `procedure_urgence.docx`
   - `controle_qualite.pdf`

### Ã‰tape 2: CrÃ©er la base de donnÃ©es vectorielle

```bash
python main.py
```

Ce script va:
1. Convertir tous les documents (Word/PDF) en texte structurÃ©
2. Appliquer le chunking hybride pour crÃ©er des chunks sÃ©mantiques
3. GÃ©nÃ©rer les embeddings pour chaque chunk
4. Stocker tout dans la base de donnÃ©es Chroma (`./chroma_db/`)

### Ã‰tape 3: Lancer l'application

```bash
python app.py
```

L'application sera accessible Ã : `http://127.0.0.1:7860`

## ğŸ“ Structure du projet

```
final-project/
â”œâ”€â”€ main.py              # Script de prÃ©paration de la base de donnÃ©es
â”œâ”€â”€ app.py               # Application web Gradio
â”œâ”€â”€ README.md            # Ce fichier
â”œâ”€â”€ docs-procedure/      # Dossier des documents source (Word / PDF)
â”‚   â”œâ”€â”€ procedure_generale.docx
â”‚   â”œâ”€â”€ securite_operationnelle.docx
â”‚   â”œâ”€â”€ maintenance_etalonage.pdf
â”‚   â”œâ”€â”€ procedure_urgence.docx
â”‚   â””â”€â”€ controle_qualite.pdf
â”œâ”€â”€ exports/             # Dossier des exports PDF/TXT
â”‚   â”œâ”€â”€ conversation_20251204_123456.pdf
â”‚   â””â”€â”€ conversation_20251204_123457.txt
â””â”€â”€ chroma_db/           # Base de donnÃ©es vectorielle (crÃ©Ã©e automatiquement)
    â””â”€â”€ chroma.sqlite3
```

## ğŸ“Š Export et Statistiques

### FonctionnalitÃ© d'Export

L'application permet d'exporter l'historique de conversation de deux faÃ§ons:

1. **Export PDF** (si reportlab est installÃ©)
   - Mise en forme professionnelle
   - Distinction des messages (utilisateur en bleu, assistant en vert)
   - Horodatage automatique
   - Stockage dans le dossier `exports/`

2. **Export TXT** (fallback)
   - Format texte brut lisible
   - Pas de dÃ©pendance externe
   - Stockage dans le dossier `exports/`

### FonctionnalitÃ© de Statistiques

Le bouton "ğŸ“Š Voir les statistiques" affiche:
- Nombre total de chunks dans la base de donnÃ©es
- Nombre de documents sources
- RÃ©partition des chunks par document (avec pourcentage)
- Utile pour comprendre la distribution des donnÃ©es

### Exemple d'affichage des statistiques

```
ğŸ“Š STATISTIQUES SUR LES DOCUMENTS

Nombre total de chunks: 1250
Nombre de documents sources: 5

RÃ©partition par document:
â€¢ procedure_generale.docx: 350 chunks (28.0%)
â€¢ maintenance_etalonage.pdf: 280 chunks (22.4%)
â€¢ controle_qualite.pdf: 260 chunks (20.8%)
â€¢ securite_operationnelle.docx: 220 chunks (17.6%)
â€¢ procedure_urgence.docx: 140 chunks (11.2%)
```

## ğŸ”§ Configuration

### Variables d'environnement Ollama

Assurez-vous que Ollama est en cours d'exÃ©cution avec les modÃ¨les requis:

```bash
# Installer les modÃ¨les (si pas dÃ©jÃ  installÃ©s)
ollama pull mxbai-embed-large
ollama pull mistral

# DÃ©marrer Ollama (si pas dÃ©jÃ  en cours)
ollama serve
```

### Installation optionnelle de reportlab

Pour obtenir des exports PDF avec une meilleure mise en forme:

```bash
pip install reportlab
```

Si reportlab n'est pas installÃ©, l'export se fera en format TXT.

## ğŸ”„ Flux de fonctionnement

```
1. Document Word â†’ Docling (conversion)
             â†“
2. Texte â†’ HybridChunker (segmentation)
             â†“
3. Chunks â†’ Ollama (embeddings)
             â†“
4. Embeddings â†’ Chroma (indexation vectorielle)
             â†“
5. RequÃªte utilisateur â†’ Recherche vectorielle
             â†“
6. Top-3 chunks + Historique â†’ Prompt Mistral
             â†“
7. RÃ©ponse formatÃ©e + Sources â†’ Interface Gradio
```

## ğŸ“– Utilisation de l'interface

### Boutons disponibles

- **Rechercher** (bleu) - Soumet la question et reÃ§oit la rÃ©ponse
- **Effacer** - Efface la question et la rÃ©ponse actuelles
- **RÃ©initialiser l'historique** - Supprime tout l'historique de conversation
- **ğŸ“¥ Exporter en PDF/TXT** - Exporte la conversation actuelle en fichier
- **ğŸ“Š Voir les statistiques** - Affiche les statistiques sur les documents indexÃ©s

### Conseils pour de meilleures rÃ©sultats

1. **Soyez prÃ©cis** - Posez des questions spÃ©cifiques sur les procÃ©dures
2. **Utilisez le contexte** - RÃ©fÃ©rencez les conversations prÃ©cÃ©dentes
3. **Consultez les sources** - VÃ©rifiez les documents utilisÃ©s pour la rÃ©ponse
4. **Exportez rÃ©guliÃ¨rement** - Sauvegardez vos conversations importantes

## ğŸ¤– ModÃ¨les utilisÃ©s

| Composant | ModÃ¨le | Fonction |
|-----------|--------|----------|
| Embeddings | `mxbai-embed-large` | Convertir le texte en vecteurs numÃ©riques |
| LLM | `mistral` | GÃ©nÃ©rer les rÃ©ponses |
| Conversion | `Docling` | Extraire le texte des documents (Word / PDF) |
| Chunking | `HybridChunker` | Segmenter les documents intelligemment |

## ğŸ› DÃ©pannage

### Erreur: "Connection refused" (Ollama)
```
Solution: VÃ©rifiez que Ollama est en cours d'exÃ©cution avec `ollama serve`
```

### Erreur: "No module named 'langchain_chroma'"
```
Solution: Installez langchain-chroma
pip install langchain-chroma
```

### Base de donnÃ©es vide
```
Solution: Assurez-vous que main.py s'est exÃ©cutÃ© sans erreurs
VÃ©rifiez que les fichiers .docx sont dans docs-procedure/
```

### RÃ©ponses lentes
```
Solution: Ottimez les paramÃ¨tres:
- RÃ©duisez k dans search_kwargs
- RÃ©duisez la taille des chunks
- VÃ©rifiez les ressources systÃ¨me
```

## ğŸ“š AmÃ©liorations possibles

- [x] Support de formats supplÃ©mentaires (PDF, TXT, etc.)
- [x] Statistiques sur les documents
- [x] Export des rÃ©ponses en PDF
- [ ] Filtrage par document source
- [ ] Interface multilingue
- [ ] Authentification utilisateur
- [ ] Historique persistant en base de donnÃ©es
- [ ] IntÃ©gration avec d'autres modÃ¨les LLM

## ğŸ“ Notes techniques

### Architecture RAG
- **Retriever**: Recherche sÃ©mantique via embeddings vectoriels
- **Augmentation**: RÃ©cupÃ©ration des top-3 chunks pertinents
- **Generation**: Utilisation du contexte pour gÃ©nÃ©rer les rÃ©ponses

### Gestion de la mÃ©moire
- Les 4 derniers messages sont conservÃ©s dans le contexte
- L'historique est stockÃ© en mÃ©moire (rÃ©initialisation Ã  chaque redÃ©marrage)
- Pas de persistance de l'historique entre les sessions

## ğŸ“„ Licence

Ce projet est fourni Ã  des fins Ã©ducatives.

## ğŸ‘¥ Auteur

Projet de Question-RÃ©ponse sur les manuels de procÃ©dures
Date: DÃ©cembre 2025

## ğŸ“ Support

Pour toute question ou problÃ¨me:
1. VÃ©rifiez la section "DÃ©pannage" ci-dessus
2. Consultez les logs d'Ollama
3. Assurez-vous que tous les prÃ©requis sont installÃ©s

## ğŸ§­ Nouveau prompt (template)

Le systÃ¨me utilise un template de prompt adaptÃ© pour interroger des manuels de procÃ©dures. Principales consignes:

- Utiliser uniquement le contexte fourni (extraits/paragraphes/sections).
- Ne pas inventer d'information ; si manquant, indiquer clairement que l'information n'a pas Ã©tÃ© trouvÃ©e.
- Pour les procÃ©dures, fournir une liste numÃ©rotÃ©e d'Ã©tapes.
- Toujours ajouter une section "Sources" avec les fichiers ou sections consultÃ©s.
- Indiquer un niveau de confiance (Ã‰levÃ© / Moyen / Faible) et la raison.
- Poser jusqu'Ã  2 questions de clarification si la requÃªte est ambigÃ¼e.

Exemple de portion du prompt utilisÃ©e par l'application:

```
Tu es un assistant expert chargÃ© d'interroger un manuel de procÃ©dures.
Utilise uniquement le CONTEXTE fourni pour rÃ©pondre (extraits, paragraphes, sections).
Ne devine pas en dehors du contexte ; si l'information n'apparaÃ®t pas dans le contexte, indique clairement
que l'information n'a pas Ã©tÃ© trouvÃ©e et propose des termes ou sections Ã  rechercher.

Consignes de rÃ©ponse :
- RÃ©sumÃ© concis (1-2 phrases) de la rÃ©ponse.
- Si la question demande une procÃ©dure, fournis une liste NUMÃ‰ROTÃ‰E des Ã©tapes Ã  suivre.
- AprÃ¨s la rÃ©ponse, ajoute une section "Sources" listant les fichiers/sections utilisÃ©s.
- Indique un court niveau de confiance (Ã‰levÃ© / Moyen / Faible) et la raison.
- Si la question est ambiguÃ« ou il manque des prÃ©cisions, pose jusqu'Ã  2 questions de clarification.
- RÃ©ponds en franÃ§ais.
```
